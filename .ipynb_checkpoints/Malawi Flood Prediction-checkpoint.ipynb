{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TNAGABA002\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\TNAGABA002\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\TNAGABA002\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\TNAGABA002\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\TNAGABA002\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\TNAGABA002\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\TNAGABA002\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\TNAGABA002\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\TNAGABA002\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\TNAGABA002\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\TNAGABA002\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\TNAGABA002\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# The following lines adjust the granularity of reporting.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "# tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>target_2015</th>\n",
       "      <th>elevation</th>\n",
       "      <th>precip2014-11-16-2014-11-23</th>\n",
       "      <th>precip2014-11-23-2014-11-30</th>\n",
       "      <th>precip2014-11-30-2014-12-07</th>\n",
       "      <th>precip2014-12-07-2014-12-14</th>\n",
       "      <th>precip2014-12-14-2014-12-21</th>\n",
       "      <th>precip2014-12-21-2014-12-28</th>\n",
       "      <th>...</th>\n",
       "      <th>precip2019-03-17-2019-03-24</th>\n",
       "      <th>precip2019-03-24-2019-03-31</th>\n",
       "      <th>precip2019-03-31-2019-04-07</th>\n",
       "      <th>precip2019-04-07-2019-04-14</th>\n",
       "      <th>precip2019-04-14-2019-04-21</th>\n",
       "      <th>precip2019-04-21-2019-04-28</th>\n",
       "      <th>precip2019-04-28-2019-05-05</th>\n",
       "      <th>precip2019-05-05-2019-05-12</th>\n",
       "      <th>precip2019-05-12-2019-05-19</th>\n",
       "      <th>LC_Type1_mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>35.1</td>\n",
       "      <td>-15.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>592.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>8.3</td>\n",
       "      <td>8.9</td>\n",
       "      <td>9.6</td>\n",
       "      <td>...</td>\n",
       "      <td>35.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.6</td>\n",
       "      <td>9.1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>10.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>354.8</td>\n",
       "      <td>4.2</td>\n",
       "      <td>8.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>14.5</td>\n",
       "      <td>3.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>34.3</td>\n",
       "      <td>-16.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>15.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>34.8</td>\n",
       "      <td>-16.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>329.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.2</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>35.0</td>\n",
       "      <td>-15.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>623.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>8.6</td>\n",
       "      <td>8.8</td>\n",
       "      <td>...</td>\n",
       "      <td>34.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.4</td>\n",
       "      <td>-15.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>751.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>...</td>\n",
       "      <td>44.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>6.4</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>35.9</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2803.3</td>\n",
       "      <td>19.4</td>\n",
       "      <td>41.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>21.8</td>\n",
       "      <td>...</td>\n",
       "      <td>72.1</td>\n",
       "      <td>16.4</td>\n",
       "      <td>37.1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>46.4</td>\n",
       "      <td>19.5</td>\n",
       "      <td>6.9</td>\n",
       "      <td>18.2</td>\n",
       "      <td>20.1</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X       Y  target_2015  elevation  precip2014-11-16-2014-11-23  \\\n",
       "count 16466.0 16466.0      16466.0    16466.0                      16466.0   \n",
       "mean     35.1   -15.8          0.1      592.8                          1.6   \n",
       "std       0.4     0.4          0.2      354.8                          4.2   \n",
       "min      34.3   -16.6          0.0       45.5                          0.0   \n",
       "25%      34.8   -16.1          0.0      329.1                          0.0   \n",
       "50%      35.0   -15.8          0.0      623.0                          0.0   \n",
       "75%      35.4   -15.5          0.0      751.4                          1.3   \n",
       "max      35.9   -15.2          1.0     2803.3                         19.4   \n",
       "\n",
       "       precip2014-11-23-2014-11-30  precip2014-11-30-2014-12-07  \\\n",
       "count                      16466.0                      16466.0   \n",
       "mean                           2.5                          1.2   \n",
       "std                            8.6                          4.4   \n",
       "min                            0.0                          0.0   \n",
       "25%                            0.0                          0.0   \n",
       "50%                            0.0                          0.0   \n",
       "75%                            0.0                          0.0   \n",
       "max                           41.0                         22.0   \n",
       "\n",
       "       precip2014-12-07-2014-12-14  precip2014-12-14-2014-12-21  \\\n",
       "count                      16466.0                      16466.0   \n",
       "mean                           8.3                          8.9   \n",
       "std                            4.3                          3.8   \n",
       "min                            1.4                          3.6   \n",
       "25%                            5.5                          5.9   \n",
       "50%                            7.9                          8.6   \n",
       "75%                           10.9                         11.0   \n",
       "max                           18.9                         23.0   \n",
       "\n",
       "       precip2014-12-21-2014-12-28  ...  precip2019-03-17-2019-03-24  \\\n",
       "count                      16466.0  ...                      16466.0   \n",
       "mean                           9.6  ...                         35.6   \n",
       "std                            4.5  ...                         14.5   \n",
       "min                            1.3  ...                         15.8   \n",
       "25%                            6.2  ...                         22.0   \n",
       "50%                            8.8  ...                         34.3   \n",
       "75%                           12.7  ...                         44.3   \n",
       "max                           21.8  ...                         72.1   \n",
       "\n",
       "       precip2019-03-24-2019-03-31  precip2019-03-31-2019-04-07  \\\n",
       "count                      16466.0                      16466.0   \n",
       "mean                           2.1                          3.5   \n",
       "std                            3.7                          8.0   \n",
       "min                            0.0                          0.0   \n",
       "25%                            0.0                          0.0   \n",
       "50%                            0.9                          0.0   \n",
       "75%                            2.1                          2.9   \n",
       "max                           16.4                         37.1   \n",
       "\n",
       "       precip2019-04-07-2019-04-14  precip2019-04-14-2019-04-21  \\\n",
       "count                      16466.0                      16466.0   \n",
       "mean                           3.6                          9.1   \n",
       "std                            3.8                          6.9   \n",
       "min                            0.0                          0.0   \n",
       "25%                            0.0                          4.4   \n",
       "50%                            2.6                          7.9   \n",
       "75%                            6.4                         13.5   \n",
       "max                           13.0                         46.4   \n",
       "\n",
       "       precip2019-04-21-2019-04-28  precip2019-04-28-2019-05-05  \\\n",
       "count                      16466.0                      16466.0   \n",
       "mean                           1.7                          0.5   \n",
       "std                            4.4                          1.5   \n",
       "min                            0.0                          0.0   \n",
       "25%                            0.0                          0.0   \n",
       "50%                            0.0                          0.0   \n",
       "75%                            0.0                          0.0   \n",
       "max                           19.5                          6.9   \n",
       "\n",
       "       precip2019-05-05-2019-05-12  precip2019-05-12-2019-05-19  LC_Type1_mode  \n",
       "count                      16466.0                      16466.0        16466.0  \n",
       "mean                           1.0                          1.6           10.7  \n",
       "std                            3.7                          4.7            2.0  \n",
       "min                            0.0                          0.0            2.0  \n",
       "25%                            0.0                          0.0            9.0  \n",
       "50%                            0.0                          0.0           10.0  \n",
       "75%                            0.0                          0.0           12.0  \n",
       "max                           18.2                         20.1           17.0  \n",
       "\n",
       "[8 rows x 39 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(filepath_or_buffer=\"train.csv\")\n",
    "train.columns = train.columns.str.replace(' ', '')\n",
    "corr_features = train[['X', 'Y', 'target_2015', 'elevation']].copy()\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = corr_features.hist(figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>target_2015</th>\n",
       "      <th>elevation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_2015</th>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elevation</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              X    Y  target_2015  elevation\n",
       "X           1.0  0.2          0.1        0.4\n",
       "Y           0.2  1.0         -0.1        0.5\n",
       "target_2015 0.1 -0.1          1.0       -0.2\n",
       "elevation   0.4  0.5         -0.2        1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_features.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "corr= corr_features.corr()\n",
    "heatmap = sns.heatmap(round(corr, 2), annot=True, ax=ax, cmap='coolwarm',fmt='.2f',linewidths=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LC_Type1_mode\n",
    "\n",
    "# 1\t05450a\tEvergreen Needleleaf Forests: dominated by evergreen conifer trees (canopy >2m). Tree cover >60%.\n",
    "# 2\t086a10\tEvergreen Broadleaf Forests: dominated by evergreen broadleaf and palmate trees (canopy >2m). Tree cover >60%.\n",
    "# 3\t54a708\tDeciduous Needleleaf Forests: dominated by deciduous needleleaf (larch) trees (canopy >2m). Tree cover >60%.\n",
    "# 4\t78d203\tDeciduous Broadleaf Forests: dominated by deciduous broadleaf trees (canopy >2m). Tree cover >60%.\n",
    "# 5\t009900\tMixed Forests: dominated by neither deciduous nor evergreen (40-60% of each) tree type (canopy >2m). Tree cover >60%.\n",
    "# 6\tc6b044\tClosed Shrublands: dominated by woody perennials (1-2m height) >60% cover.\n",
    "# 7\tdcd159\tOpen Shrublands: dominated by woody perennials (1-2m height) 10-60% cover.\n",
    "# 8\tdade48\tWoody Savannas: tree cover 30-60% (canopy >2m).\n",
    "# 9\tfbff13\tSavannas: tree cover 10-30% (canopy >2m).\n",
    "# 10\tb6ff05\tGrasslands: dominated by herbaceous annuals (<2m).\n",
    "# 11\t27ff87\tPermanent Wetlands: permanently inundated lands with 30-60% water cover and >10% vegetated cover.\n",
    "# 12\tc24f44\tCroplands: at least 60% of area is cultivated cropland.\n",
    "# 13\ta5a5a5\tUrban and Built-up Lands: at least 30% impervious surface area including building materials, asphalt and vehicles.\n",
    "# 14\tff6d4c\tCropland/Natural Vegetation Mosaics: mosaics of small-scale cultivation 40-60% with natural tree, shrub, or herbaceous vegetation.\n",
    "# 15\t69fff8\tPermanent Snow and Ice: at least 60% of area is covered by snow and ice for at least 10 months of the year.\n",
    "# 16\tf9ffa4\tBarren: at least 60% of area is non-vegetated barren (sand, rock, soil) areas with less than 10% vegetation.\n",
    "# 17\t1c0dff\tWater Bodies: at least 60% of area is covered by permanent water bodies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>target_2015</th>\n",
       "      <th>elevation</th>\n",
       "      <th>precip2014-11-16-2014-11-23</th>\n",
       "      <th>precip2014-11-23-2014-11-30</th>\n",
       "      <th>precip2014-11-30-2014-12-07</th>\n",
       "      <th>precip2014-12-07-2014-12-14</th>\n",
       "      <th>precip2014-12-14-2014-12-21</th>\n",
       "      <th>precip2014-12-21-2014-12-28</th>\n",
       "      <th>...</th>\n",
       "      <th>precip2019-03-24-2019-03-31</th>\n",
       "      <th>precip2019-03-31-2019-04-07</th>\n",
       "      <th>precip2019-04-07-2019-04-14</th>\n",
       "      <th>precip2019-04-14-2019-04-21</th>\n",
       "      <th>precip2019-04-21-2019-04-28</th>\n",
       "      <th>precip2019-04-28-2019-05-05</th>\n",
       "      <th>precip2019-05-05-2019-05-12</th>\n",
       "      <th>precip2019-05-12-2019-05-19</th>\n",
       "      <th>LC_Type1_mode</th>\n",
       "      <th>Square_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.3</td>\n",
       "      <td>-15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>887.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>14.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>4e3c3896-14ce-11ea-bce5-f49634744a41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.3</td>\n",
       "      <td>-15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>743.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>14.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>4e3c3897-14ce-11ea-bce5-f49634744a41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.3</td>\n",
       "      <td>-15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>565.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>14.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>4e3c3898-14ce-11ea-bce5-f49634744a41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34.3</td>\n",
       "      <td>-15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>14.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4e3c3899-14ce-11ea-bce5-f49634744a41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.3</td>\n",
       "      <td>-15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>437.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>14.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4e3c389a-14ce-11ea-bce5-f49634744a41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     X     Y  target_2015  elevation  precip2014-11-16-2014-11-23  \\\n",
       "0 34.3 -15.9          0.0      887.8                          0.0   \n",
       "1 34.3 -15.9          0.0      743.4                          0.0   \n",
       "2 34.3 -15.9          0.0      565.7                          0.0   \n",
       "3 34.3 -15.9          0.0      443.4                          0.0   \n",
       "4 34.3 -15.9          0.0      437.4                          0.0   \n",
       "\n",
       "   precip2014-11-23-2014-11-30  precip2014-11-30-2014-12-07  \\\n",
       "0                          0.0                          0.0   \n",
       "1                          0.0                          0.0   \n",
       "2                          0.0                          0.0   \n",
       "3                          0.0                          0.0   \n",
       "4                          0.0                          0.0   \n",
       "\n",
       "   precip2014-12-07-2014-12-14  precip2014-12-14-2014-12-21  \\\n",
       "0                         14.8                         14.6   \n",
       "1                         14.8                         14.6   \n",
       "2                         14.8                         14.6   \n",
       "3                         14.8                         14.6   \n",
       "4                         14.8                         14.6   \n",
       "\n",
       "   precip2014-12-21-2014-12-28  ...  precip2019-03-24-2019-03-31  \\\n",
       "0                         12.2  ...                          0.9   \n",
       "1                         12.2  ...                          0.9   \n",
       "2                         12.2  ...                          0.9   \n",
       "3                         12.2  ...                          0.9   \n",
       "4                         12.2  ...                          0.9   \n",
       "\n",
       "   precip2019-03-31-2019-04-07  precip2019-04-07-2019-04-14  \\\n",
       "0                          1.7                          0.0   \n",
       "1                          1.7                          0.0   \n",
       "2                          1.7                          0.0   \n",
       "3                          1.7                          0.0   \n",
       "4                          1.7                          0.0   \n",
       "\n",
       "   precip2019-04-14-2019-04-21  precip2019-04-21-2019-04-28  \\\n",
       "0                          0.0                          0.0   \n",
       "1                          0.0                          0.0   \n",
       "2                          0.0                          0.0   \n",
       "3                          0.0                          0.0   \n",
       "4                          0.0                          0.0   \n",
       "\n",
       "   precip2019-04-28-2019-05-05  precip2019-05-05-2019-05-12  \\\n",
       "0                          0.0                          0.0   \n",
       "1                          0.0                          0.0   \n",
       "2                          0.0                          0.0   \n",
       "3                          0.0                          0.0   \n",
       "4                          0.0                          0.0   \n",
       "\n",
       "   precip2019-05-12-2019-05-19  LC_Type1_mode  \\\n",
       "0                          0.0              9   \n",
       "1                          0.0              9   \n",
       "2                          0.0              9   \n",
       "3                          0.0             10   \n",
       "4                          0.0             10   \n",
       "\n",
       "                              Square_ID  \n",
       "0  4e3c3896-14ce-11ea-bce5-f49634744a41  \n",
       "1  4e3c3897-14ce-11ea-bce5-f49634744a41  \n",
       "2  4e3c3898-14ce-11ea-bce5-f49634744a41  \n",
       "3  4e3c3899-14ce-11ea-bce5-f49634744a41  \n",
       "4  4e3c389a-14ce-11ea-bce5-f49634744a41  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross 'X' and 'Y'\n",
    "train_df = train.copy()\n",
    "train_df['XY'] = train_df['X']*train_df['Y']\n",
    "train_df[\"XY_elevation\"] = train_df['XY'] * train_df['elevation']\n",
    "train_df.describe()\n",
    "\n",
    "# divide 'X' and 'Y'\n",
    "train_df['X/Y'] = train_df['X']/train_df['Y']\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCalculate slope\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Calculate slope\n",
    "'''\n",
    "# slope_X = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>target_2015</th>\n",
       "      <th>elevation</th>\n",
       "      <th>precip2014-11-16-2014-11-23</th>\n",
       "      <th>precip2014-11-23-2014-11-30</th>\n",
       "      <th>precip2014-11-30-2014-12-07</th>\n",
       "      <th>precip2014-12-07-2014-12-14</th>\n",
       "      <th>precip2014-12-14-2014-12-21</th>\n",
       "      <th>precip2014-12-21-2014-12-28</th>\n",
       "      <th>...</th>\n",
       "      <th>precip2019-05-12-2019-05-19</th>\n",
       "      <th>LC_Type1_mode</th>\n",
       "      <th>XY</th>\n",
       "      <th>XY_elevation</th>\n",
       "      <th>X/Y</th>\n",
       "      <th>annual_precip_2015</th>\n",
       "      <th>annual_precip_2019</th>\n",
       "      <th>total_precip</th>\n",
       "      <th>ave_precip_2015</th>\n",
       "      <th>ave_precip_2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "      <td>16466.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>35.1</td>\n",
       "      <td>-15.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>592.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>8.3</td>\n",
       "      <td>8.9</td>\n",
       "      <td>9.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6</td>\n",
       "      <td>10.7</td>\n",
       "      <td>-554.7</td>\n",
       "      <td>-327752.4</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>194.0</td>\n",
       "      <td>213.5</td>\n",
       "      <td>-462.4</td>\n",
       "      <td>19.4</td>\n",
       "      <td>12.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>354.8</td>\n",
       "      <td>4.2</td>\n",
       "      <td>8.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>197226.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>23.7</td>\n",
       "      <td>27.2</td>\n",
       "      <td>57.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>34.3</td>\n",
       "      <td>-16.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-585.7</td>\n",
       "      <td>-1591324.7</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>148.2</td>\n",
       "      <td>175.4</td>\n",
       "      <td>-716.8</td>\n",
       "      <td>14.8</td>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>34.8</td>\n",
       "      <td>-16.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>329.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-564.7</td>\n",
       "      <td>-417132.0</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>179.3</td>\n",
       "      <td>199.3</td>\n",
       "      <td>-466.1</td>\n",
       "      <td>17.9</td>\n",
       "      <td>11.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>35.0</td>\n",
       "      <td>-15.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>623.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>8.6</td>\n",
       "      <td>8.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-554.3</td>\n",
       "      <td>-341942.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>189.0</td>\n",
       "      <td>207.6</td>\n",
       "      <td>-449.4</td>\n",
       "      <td>18.9</td>\n",
       "      <td>12.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.4</td>\n",
       "      <td>-15.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>751.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-544.6</td>\n",
       "      <td>-181173.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>206.7</td>\n",
       "      <td>220.6</td>\n",
       "      <td>-436.9</td>\n",
       "      <td>20.7</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>35.9</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2803.3</td>\n",
       "      <td>19.4</td>\n",
       "      <td>41.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>21.8</td>\n",
       "      <td>...</td>\n",
       "      <td>20.1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-526.0</td>\n",
       "      <td>-26592.7</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>251.6</td>\n",
       "      <td>323.8</td>\n",
       "      <td>-396.0</td>\n",
       "      <td>25.2</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X       Y  target_2015  elevation  precip2014-11-16-2014-11-23  \\\n",
       "count 16466.0 16466.0      16466.0    16466.0                      16466.0   \n",
       "mean     35.1   -15.8          0.1      592.8                          1.6   \n",
       "std       0.4     0.4          0.2      354.8                          4.2   \n",
       "min      34.3   -16.6          0.0       45.5                          0.0   \n",
       "25%      34.8   -16.1          0.0      329.1                          0.0   \n",
       "50%      35.0   -15.8          0.0      623.0                          0.0   \n",
       "75%      35.4   -15.5          0.0      751.4                          1.3   \n",
       "max      35.9   -15.2          1.0     2803.3                         19.4   \n",
       "\n",
       "       precip2014-11-23-2014-11-30  precip2014-11-30-2014-12-07  \\\n",
       "count                      16466.0                      16466.0   \n",
       "mean                           2.5                          1.2   \n",
       "std                            8.6                          4.4   \n",
       "min                            0.0                          0.0   \n",
       "25%                            0.0                          0.0   \n",
       "50%                            0.0                          0.0   \n",
       "75%                            0.0                          0.0   \n",
       "max                           41.0                         22.0   \n",
       "\n",
       "       precip2014-12-07-2014-12-14  precip2014-12-14-2014-12-21  \\\n",
       "count                      16466.0                      16466.0   \n",
       "mean                           8.3                          8.9   \n",
       "std                            4.3                          3.8   \n",
       "min                            1.4                          3.6   \n",
       "25%                            5.5                          5.9   \n",
       "50%                            7.9                          8.6   \n",
       "75%                           10.9                         11.0   \n",
       "max                           18.9                         23.0   \n",
       "\n",
       "       precip2014-12-21-2014-12-28  ...  precip2019-05-12-2019-05-19  \\\n",
       "count                      16466.0  ...                      16466.0   \n",
       "mean                           9.6  ...                          1.6   \n",
       "std                            4.5  ...                          4.7   \n",
       "min                            1.3  ...                          0.0   \n",
       "25%                            6.2  ...                          0.0   \n",
       "50%                            8.8  ...                          0.0   \n",
       "75%                           12.7  ...                          0.0   \n",
       "max                           21.8  ...                         20.1   \n",
       "\n",
       "       LC_Type1_mode      XY  XY_elevation     X/Y  annual_precip_2015  \\\n",
       "count        16466.0 16466.0       16466.0 16466.0             16466.0   \n",
       "mean            10.7  -554.7     -327752.4    -2.2               194.0   \n",
       "std              2.0    12.8      197226.3     0.1                23.7   \n",
       "min              2.0  -585.7    -1591324.7    -2.4               148.2   \n",
       "25%              9.0  -564.7     -417132.0    -2.3               179.3   \n",
       "50%             10.0  -554.3     -341942.2    -2.2               189.0   \n",
       "75%             12.0  -544.6     -181173.2    -2.2               206.7   \n",
       "max             17.0  -526.0      -26592.7    -2.1               251.6   \n",
       "\n",
       "       annual_precip_2019  total_precip  ave_precip_2015  ave_precip_2019  \n",
       "count             16466.0       16466.0          16466.0          16466.0  \n",
       "mean                213.5        -462.4             19.4             12.6  \n",
       "std                  27.2          57.6              2.4              1.6  \n",
       "min                 175.4        -716.8             14.8             10.3  \n",
       "25%                 199.3        -466.1             17.9             11.7  \n",
       "50%                 207.6        -449.4             18.9             12.2  \n",
       "75%                 220.6        -436.9             20.7             13.0  \n",
       "max                 323.8        -396.0             25.2             19.0  \n",
       "\n",
       "[8 rows x 47 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sum of, total yearly precip and average precip fields\n",
    "\n",
    "train_df['annual_precip_2015'] = 0\n",
    "train_df['annual_precip_2019'] = 0\n",
    "train_df['total_precip'] = 0\n",
    "count_2015=0\n",
    "count_2019=0\n",
    "for col in train_df.columns:\n",
    "    if len(col) == 27:\n",
    "        train_df['total_precip'] = train_df['total_precip'] +- train_df[col]\n",
    "        if col[9] == \"5\":\n",
    "            count_2015 += 1\n",
    "            train_df['annual_precip_2015'] += train_df[col]\n",
    "        elif  col[9] == \"9\":\n",
    "            count_2019 += 1\n",
    "            train_df['annual_precip_2019'] += train_df[col]\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        continue\n",
    "train_df[\"ave_precip_2015\"] = train_df['annual_precip_2015'] / count_2015\n",
    "train_df[\"ave_precip_2019\"] = train_df['annual_precip_2019'] / count_2019\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>target_2015</th>\n",
       "      <th>elevation</th>\n",
       "      <th>precip2014-11-16-2014-11-23</th>\n",
       "      <th>precip2014-11-23-2014-11-30</th>\n",
       "      <th>precip2014-11-30-2014-12-07</th>\n",
       "      <th>precip2014-12-07-2014-12-14</th>\n",
       "      <th>precip2014-12-14-2014-12-21</th>\n",
       "      <th>precip2014-12-21-2014-12-28</th>\n",
       "      <th>...</th>\n",
       "      <th>precip2019-03-24-2019-03-31</th>\n",
       "      <th>precip2019-03-31-2019-04-07</th>\n",
       "      <th>precip2019-04-07-2019-04-14</th>\n",
       "      <th>precip2019-04-14-2019-04-21</th>\n",
       "      <th>precip2019-04-21-2019-04-28</th>\n",
       "      <th>precip2019-04-28-2019-05-05</th>\n",
       "      <th>precip2019-05-05-2019-05-12</th>\n",
       "      <th>precip2019-05-12-2019-05-19</th>\n",
       "      <th>LC_Type1_mode</th>\n",
       "      <th>Square_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.3</td>\n",
       "      <td>-15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>887.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>14.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>4e3c3896-14ce-11ea-bce5-f49634744a41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.3</td>\n",
       "      <td>-15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>743.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>14.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>4e3c3897-14ce-11ea-bce5-f49634744a41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.3</td>\n",
       "      <td>-15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>565.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>14.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>4e3c3898-14ce-11ea-bce5-f49634744a41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34.3</td>\n",
       "      <td>-15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>14.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4e3c3899-14ce-11ea-bce5-f49634744a41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.3</td>\n",
       "      <td>-15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>437.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>14.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4e3c389a-14ce-11ea-bce5-f49634744a41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     X     Y  target_2015  elevation  precip2014-11-16-2014-11-23  \\\n",
       "0 34.3 -15.9          0.0      887.8                          0.0   \n",
       "1 34.3 -15.9          0.0      743.4                          0.0   \n",
       "2 34.3 -15.9          0.0      565.7                          0.0   \n",
       "3 34.3 -15.9          0.0      443.4                          0.0   \n",
       "4 34.3 -15.9          0.0      437.4                          0.0   \n",
       "\n",
       "   precip2014-11-23-2014-11-30  precip2014-11-30-2014-12-07  \\\n",
       "0                          0.0                          0.0   \n",
       "1                          0.0                          0.0   \n",
       "2                          0.0                          0.0   \n",
       "3                          0.0                          0.0   \n",
       "4                          0.0                          0.0   \n",
       "\n",
       "   precip2014-12-07-2014-12-14  precip2014-12-14-2014-12-21  \\\n",
       "0                         14.8                         14.6   \n",
       "1                         14.8                         14.6   \n",
       "2                         14.8                         14.6   \n",
       "3                         14.8                         14.6   \n",
       "4                         14.8                         14.6   \n",
       "\n",
       "   precip2014-12-21-2014-12-28  ...  precip2019-03-24-2019-03-31  \\\n",
       "0                         12.2  ...                          0.9   \n",
       "1                         12.2  ...                          0.9   \n",
       "2                         12.2  ...                          0.9   \n",
       "3                         12.2  ...                          0.9   \n",
       "4                         12.2  ...                          0.9   \n",
       "\n",
       "   precip2019-03-31-2019-04-07  precip2019-04-07-2019-04-14  \\\n",
       "0                          1.7                          0.0   \n",
       "1                          1.7                          0.0   \n",
       "2                          1.7                          0.0   \n",
       "3                          1.7                          0.0   \n",
       "4                          1.7                          0.0   \n",
       "\n",
       "   precip2019-04-14-2019-04-21  precip2019-04-21-2019-04-28  \\\n",
       "0                          0.0                          0.0   \n",
       "1                          0.0                          0.0   \n",
       "2                          0.0                          0.0   \n",
       "3                          0.0                          0.0   \n",
       "4                          0.0                          0.0   \n",
       "\n",
       "   precip2019-04-28-2019-05-05  precip2019-05-05-2019-05-12  \\\n",
       "0                          0.0                          0.0   \n",
       "1                          0.0                          0.0   \n",
       "2                          0.0                          0.0   \n",
       "3                          0.0                          0.0   \n",
       "4                          0.0                          0.0   \n",
       "\n",
       "   precip2019-05-12-2019-05-19  LC_Type1_mode  \\\n",
       "0                          0.0              9   \n",
       "1                          0.0              9   \n",
       "2                          0.0              9   \n",
       "3                          0.0             10   \n",
       "4                          0.0             10   \n",
       "\n",
       "                              Square_ID  \n",
       "0  4e3c3896-14ce-11ea-bce5-f49634744a41  \n",
       "1  4e3c3897-14ce-11ea-bce5-f49634744a41  \n",
       "2  4e3c3898-14ce-11ea-bce5-f49634744a41  \n",
       "3  4e3c3899-14ce-11ea-bce5-f49634744a41  \n",
       "4  4e3c389a-14ce-11ea-bce5-f49634744a41  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Feature_Name  Score\n",
      "5   precip2014-11-30-2014-12-07    2.7\n",
      "35  precip2019-05-05-2019-05-12    2.6\n",
      "39                 XY_elevation    2.6\n",
      "2                     elevation    2.6\n",
      "19  precip2015-03-08-2015-03-15    2.6\n",
      "..                          ...    ...\n",
      "37                LC_Type1_mode    0.7\n",
      "31  precip2019-04-07-2019-04-14    0.7\n",
      "21  precip2019-01-27-2019-02-03    0.7\n",
      "22  precip2019-02-03-2019-02-10    0.6\n",
      "7   precip2014-12-14-2014-12-21    0.6\n",
      "\n",
      "[46 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Feature importance ranking\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "x = train_df.drop(['target_2015', 'Square_ID'] , axis = 1)\n",
    "y = train_df['target_2015']\n",
    "\n",
    "num_of_cols = len(x.columns)\n",
    "\n",
    "# Keep 5 features\n",
    "selector = SelectKBest(f_classif, k=num_of_cols)\n",
    "\n",
    "fit = selector.fit(x, y)\n",
    "\n",
    "df_scores = pd.DataFrame(fit.scores_)\n",
    "df_columns = pd.DataFrame(x.columns)\n",
    "\n",
    "# concatenate dataframes\n",
    "feature_scores = pd.concat([df_columns, df_scores],axis=1)\n",
    "feature_scores.columns = ['Feature_Name','Score']  # name output columns\n",
    "print(feature_scores.nlargest(num_of_cols,'Score'))  # print 20 best features\n",
    "feature_scores.head(num_of_cols)\n",
    "\n",
    "# export selected features to .csv\n",
    "df_univ_feat = feature_scores.nlargest(num_of_cols,'Score')\n",
    "df_univ_feat.to_csv('feature_selection_UNIVARIATE.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(*args):\n",
    "    for arg in args:\n",
    "        feature_name = 'norm_'+arg\n",
    "        train_df[feature_name] = (train_df[arg].copy() - train_df[arg].mean())/train_df[arg].std()\n",
    "#         test_df[feature_name] = (test_df[arg].copy() - test_df[arg].mean())/test_df[arg].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NumericColumn(key='norm_elevation', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_XY', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_XY_elevation', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_total_precip', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_LC_Type1_mode', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_X/Y', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_precip2014-11-30-2014-12-07', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_precip2019-05-05-2019-05-12', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_precip2015-03-08-2015-03-15', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_precip2019-02-24-2019-03-03', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_precip2014-11-23-2014-11-30', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_precip2019-02-10-2019-02-17', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_precip2019-04-21-2019-04-28', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_precip2019-03-24-2019-03-31', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_precip2019-03-31-2019-04-07', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_precip2014-11-16-2014-11-23', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_precip2019-05-12-2019-05-19', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_precip2019-04-28-2019-05-05', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='norm_precip2019-03-17-2019-03-24', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_features/concat:0' shape=(16466, 19) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "PLEASE ONLY RUN THIS ONCE PER TRAINING\n",
    "It calls the create validation split which will resplit the data if run multiple times\n",
    "'''\n",
    "# Create an empty list that will eventually hold all created feature columns.\n",
    "feature_columns = []\n",
    "\n",
    "# Invoke the normalize feature method\n",
    "normalize_feature(train_df.drop([\n",
    "                                'target_2015', \n",
    "                                'Square_ID'\n",
    "                                ] , axis = 1).columns)\n",
    "\n",
    "# Define features to include\n",
    "my_cols = ['norm_elevation', 'norm_XY', 'norm_XY_elevation', 'norm_total_precip', 'norm_LC_Type1_mode', 'norm_X/Y', \n",
    "           'norm_precip2014-11-30-2014-12-07', 'norm_precip2019-05-05-2019-05-12', 'norm_precip2015-03-08-2015-03-15', \n",
    "           'norm_precip2019-02-24-2019-03-03', 'norm_precip2014-11-23-2014-11-30', 'norm_precip2019-02-10-2019-02-17', \n",
    "           'norm_precip2019-04-21-2019-04-28', 'norm_precip2019-03-24-2019-03-31', 'norm_precip2019-03-31-2019-04-07', \n",
    "           'norm_precip2014-11-16-2014-11-23', 'norm_precip2019-05-12-2019-05-19', 'norm_precip2019-04-28-2019-05-05', \n",
    "           'norm_precip2019-03-17-2019-03-24'\n",
    "          ]\n",
    "for col in my_cols:\n",
    "    feature = tf.feature_column.numeric_column(col)\n",
    "    feature_columns.append(feature)\n",
    "\n",
    "# # Create a numerical feature column to represent feature 1.\n",
    "# feature_1 = tf.feature_column.numeric_column(\"norm_elevation\")\n",
    "# feature_columns.append(feature_1)\n",
    "\n",
    "# # Create a numerical feature column to represent feature 2.\n",
    "# feature_2 = tf.feature_column.numeric_column(\"norm_XY\")\n",
    "# feature_columns.append(feature_2)\n",
    "\n",
    "# # Create a numerical feature column to represent longitude.\n",
    "# feature_3 = tf.feature_column.numeric_column(\"norm_XY_elevation\")\n",
    "# feature_columns.append(feature_3)\n",
    "\n",
    "# # Create a numerical feature column to represent longitude.\n",
    "# feature_4 = tf.feature_column.numeric_column(\"norm_total_precip\")\n",
    "# feature_columns.append(feature_4)\n",
    "\n",
    "# # Create a numerical feature column to represent longitude.\n",
    "# feature_5 = tf.feature_column.numeric_column(\"norm_LC_Type1_mode\")\n",
    "# feature_columns.append(feature_5)\n",
    "\n",
    "# # Create a numerical feature column to represent longitude.\n",
    "# feature_6 = tf.feature_column.numeric_column(\"norm_X/Y\")\n",
    "# feature_columns.append(feature_6)\n",
    "\n",
    "print(feature_columns)\n",
    "\n",
    "# Convert the list of feature columns into a layer that will later be fed into\n",
    "# the model. \n",
    "feature_layer = layers.DenseFeatures(feature_columns)\n",
    "\n",
    "# Print the first 3 and last 3 rows of the feature_layer's output when applied\n",
    "# to train_df_norm:\n",
    "feature_layer(dict(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the plot_the_model and plot_the_loss_curve functions.\n"
     ]
    }
   ],
   "source": [
    "#@title Define the plotting functions\n",
    "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
    "  \"\"\"Plot the trained model against 200 random training examples.\"\"\"\n",
    "\n",
    "  # Label the axes.\n",
    "  plt.xlabel(feature)\n",
    "  plt.ylabel(label)\n",
    "\n",
    "  # Create a scatter plot from 200 random points of the dataset.\n",
    "  random_examples = train.sample(n=200)\n",
    "  plt.scatter(random_examples[feature], random_examples[label])\n",
    "\n",
    "  # Create a red line representing the model. The red line starts\n",
    "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
    "  x0 = -2.5\n",
    "  y0 = trained_bias\n",
    "  x1 = 2.5\n",
    "  y1 = trained_bias + (trained_weight * x1)\n",
    "  plt.plot([x0, x1], [y0, y1], c='r')\n",
    "\n",
    "  # Render the scatter plot and the red line.\n",
    "  plt.show()\n",
    "    \n",
    "\n",
    "# Needs to be updated to plot the line of best fit\n",
    "#@title Define the plotting functions\n",
    "def plot_the_model_plotly(trained_weight_1, trained_weight_2, trained_bias, feature_1, feature_2, label):\n",
    "  \"\"\"Plot the trained model against 200 random training examples.\"\"\"\n",
    "\n",
    "#   # Label the axes.\n",
    "#   plt.xlabel(feature_1)\n",
    "#   plt.xlabel(feature_2)\n",
    "#   plt.ylabel(label)\n",
    "\n",
    "\n",
    "  # Create a surface plot representing the model\n",
    "  x, y = np.linspace(0, -10, 20).reshape(4, 5), np.linspace(0, -10, 20).reshape(4, 5)\n",
    "  z = trained_bias + (trained_weight_1 * x) + (trained_weight_2 * y)\n",
    "  fig = go.Figure(data=[go.Surface(z=z), go.Scatter3d(x=train[feature_1], y=train[feature_2], z=train[label],\n",
    "                                   mode='markers')])\n",
    "  fig.update_layout(title='Fit and scatter', autosize=True,\n",
    "                      width=500, height=500,\n",
    "                      margin=dict(l=65, r=50, b=65, t=90))\n",
    "\n",
    "    \n",
    "#   # Create a red line representing the model. The red line starts\n",
    "#   # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
    "#   x0 = -16.8\n",
    "#   y0 = trained_bias\n",
    "#   x1 = -15.2\n",
    "#   x2 = 35.5\n",
    "#   y1 = trained_bias + (trained_weight_1 * x1) + (trained_weight_2 * x2)\n",
    "#   plt.plot([x0, x1], [y0, y1], c='r')\n",
    "\n",
    "  # Render the scatter plot and the red line.\n",
    "  fig.show()\n",
    "\n",
    "\n",
    "def plot_the_loss_curve(epochs, rmse):\n",
    "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Root Mean Squared Error\")\n",
    "\n",
    "  plt.plot(epochs, rmse, label=\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.ylim([rmse.min()*0.95, rmse.max()*1.03])\n",
    "  plt.show()  \n",
    "\n",
    "print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the create_model and traing_model functions.\n"
     ]
    }
   ],
   "source": [
    "#@title Define the functions that build and train a model\n",
    "def build_model(my_learning_rate):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "  # Most simple tf.keras models are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Add the feature layer (the list of features and how they are represented)\n",
    "  # to the model.\n",
    "  model.add(feature_layer)\n",
    "    \n",
    "#   ## Add dropout layer\n",
    "#   model.add(tf.keras.layers.Dropout(0.1))\n",
    "    \n",
    "  # Describe the topography of the model.\n",
    "  # The topography of a simple linear regression model\n",
    "  # is a single node in a single layer.\n",
    "  model.add(tf.keras.layers.Dense(units=1, \n",
    "                                  input_shape=(1,),\n",
    "                                  activation='relu',\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l1(l=0.00001)))\n",
    "  \n",
    "#   # Define the output layer.\n",
    "#   model.add(tf.keras.layers.Dense(units=1,  \n",
    "#                                   name='Output'))\n",
    "\n",
    "  # Compile the model topography into code that TensorFlow can efficiently\n",
    "  # execute. Configure training to minimize the model's mean squared error. \n",
    "  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "  return model    \n",
    "\n",
    "\n",
    "def train_model(model, dataset, validate_set, label_name, epochs, batch_size, shuffle=True, my_validation_split=0.2):\n",
    "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "  # The x parameter of tf.keras.Model.fit can be a list of arrays, where\n",
    "  # each array contains the data for one feature.  Here, we're passing\n",
    "  # every column in the dataset. Note that the feature_layer will filter\n",
    "  # away most of those columns, leaving only the desired columns and their\n",
    "  # representations as features.\n",
    "  features = {name:np.array(value) for name, value in dataset.items()}\n",
    "  label = np.array(features.pop(label_name))\n",
    "    \n",
    "  # Create callback for early stop\n",
    "  # Create checkpoint at that point\n",
    "  # I have added val before root_mean_square to capture this\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='root_mean_squared_error', verbose=1, patience=100)\n",
    "  checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='root_mean_squared_error', verbose=1, save_best_only=True)\n",
    "    \n",
    "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle, \n",
    "#                       validation_split = 0.2,\n",
    "                      callbacks= [early_stopping, checkpoint]\n",
    "                     )\n",
    "\n",
    "  # Gather the trained model's weight and bias.\n",
    "  print(model.get_weights())\n",
    "  trained_weight = model.get_weights()[0]\n",
    "  trained_bias = model.get_weights()[1]\n",
    "\n",
    "  # The list of epochs is stored separately from the rest of history.\n",
    "  epochs_hist = history.epoch\n",
    "  \n",
    "  # Isolate the error for each epoch.\n",
    "  hist = pd.DataFrame(history.history)\n",
    "\n",
    "  # To track the progression of training, we're going to take a snapshot\n",
    "  # of the model's root mean squared error at each epoch. \n",
    "  rmse = hist[\"root_mean_squared_error\"]\n",
    "\n",
    "  # Score on validation set\n",
    "  validate_features = {name:np.array(value) for name, value in validate_set.items()}\n",
    "  validate_label = np.array(validate_features.pop(my_label))\n",
    "  \n",
    "  model.evaluate(x = validate_features, y = validate_label, batch_size=batch_size)\n",
    "\n",
    "  return trained_weight, trained_bias, epochs_hist, rmse\n",
    "\n",
    "print(\"Defined the create_model and traing_model functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport talos\\nfrom talos.utils import lr_normalizer\\nfrom sklearn.model_selection import train_test_split\\n\\nx, y = talos.templates.datasets.breast_cancer()\\n\\nprint(type(x), type(y))\\n\\n# Create validation split\\nx_train, x_val, y_train, y_val = train_test_split(train_df, train_df[\\'target_2015\\'], test_size=0.2, random_state=42)\\nprint(x_train.shape)\\nprint(y_train.shape)\\n\\nfeatures_train = {name:np.array(value) for name, value in x_train.items()}\\nlabel_train = np.array(features_train.pop(\\'target_2015\\'))\\nfeatures_val = {name:np.array(value) for name, value in x_val.items()}\\nlabel_val = np.array(features_val.pop(\\'target_2015\\'))\\n\\nprint(type(x_train.to_numpy()), type(label_train))\\n\\n# define label\\nmy_label = \\'target_2015\\'\\n\\n# first we have to make sure to input data and params into the function\\ndef build_talos_model(x_train, y_train, features_val, label_val, params):\\n    model = tf.keras.Sequential()\\n    \\n    # Add the feature layer (the list of features and how they are represented)\\n    # to the model.\\n    model.add(feature_layer)\\n\\n    model.add(tf.keras.layers.Dense(params[\\'first_neuron\\'], input_dim=x_train.shape[1],\\n                    activation=params[\\'activation\\'],\\n                    kernel_initializer=params[\\'kernel_initializer\\']))\\n    \\n    model.add(tf.keras.layers.Dropout(params[\\'dropout\\']))\\n\\n    model.add(tf.keras.layers.Dense(1, activation=params[\\'last_activation\\'],\\n                    kernel_initializer=params[\\'kernel_initializer\\']))\\n    \\n    model.compile(loss=params[\\'losses\\'],\\n                  optimizer=params[\\'optimizer\\'](lr=lr_normalizer(params[\\'lr\\'], params[\\'optimizer\\'])),\\n                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\\n    \\n    \\n    \\n#     # Manual kfold cross_validation\\n#     kfold = KFold(n_splits=params[num_of_splits], shuffle=True, random_state=7)\\n#     for train_index, validate_index in kfold.split(train_df, train_df[\\'target_2015\\']):\\n#         print(train_index)\\n#         print(validate_index)\\n#         weight, bias, epochs_hist, rmse = train_model(my_model, train_df.iloc[train_index, :], train_df.iloc[validate_index, :], my_label,\\n#                                                  params[epochs], params[batch_size])\\n\\n    # Create callback for early stop\\n    # Create checkpoint at that point\\n    # I have added val before root_mean_square to capture this\\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\\'root_mean_squared_error\\', verbose=1, patience=100)\\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\\'best_model.h5\\', monitor=\\'root_mean_squared_error\\', verbose=1, save_best_only=True)\\n    \\n    history = model.fit(x_train, y_train, \\n                        validation_data=[x_val, y_val],\\n                        batch_size=params[\\'batch_size\\'],\\n                        callbacks=[talos.utils.live()],\\n                        epochs=params[\\'epochs\\'],\\n                        verbose=0)\\n\\n    print(\"\\nAll weights for your model are: \"+ str(weight))\\n    print(\"The learned bias for your model is %.4f\\n\" % bias )\\n\\n    plot_the_loss_curve(epochs_hist, rmse)\\n\\n    return history, model\\n\\n# then we can go ahead and set the parameter space\\np = {\\'first_neuron\\':[1,2,3, 4, 5],\\n     \\'hidden_layers\\':[0, 1, 2],\\n     \\'lr\\':[0.05, 0.01, 0.005, 0.001, 0.0001],\\n     \\'batch_size\\': [30],\\n     \\'epochs\\': [100],\\n     \\'dropout\\': [0],\\n     \\'num_of_splits\\': [2, 5, 8, 10],\\n     \\'kernel_initializer\\': [\\'uniform\\',\\'normal\\'],\\n     \\'optimizer\\': [tf.keras.optimizers.Nadam, tf.keras.optimizers.Adam, tf.keras.optimizers.RMSprop],\\n     \\'losses\\': [\\'mean_squared_error\\'],\\n     \\'activation\\':[\\'relu\\', \\'elu\\'],\\n     \\'last_activation\\': [\\'sigmoid\\']}\\n\\n\\n# and run the experiment\\nt = talos.Scan(x=x_train.to_numpy(),\\n               y=label_train,\\n               model=build_talos_model,\\n               params=p,\\n               experiment_name=\\'talos_tuning\\',\\n               round_limit=10\\n              )\\nprint(t)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Alternative using talos\n",
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "import talos\n",
    "from talos.utils import lr_normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x, y = talos.templates.datasets.breast_cancer()\n",
    "\n",
    "print(type(x), type(y))\n",
    "\n",
    "# Create validation split\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_df, train_df['target_2015'], test_size=0.2, random_state=42)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "features_train = {name:np.array(value) for name, value in x_train.items()}\n",
    "label_train = np.array(features_train.pop('target_2015'))\n",
    "features_val = {name:np.array(value) for name, value in x_val.items()}\n",
    "label_val = np.array(features_val.pop('target_2015'))\n",
    "\n",
    "print(type(x_train.to_numpy()), type(label_train))\n",
    "\n",
    "# define label\n",
    "my_label = 'target_2015'\n",
    "\n",
    "# first we have to make sure to input data and params into the function\n",
    "def build_talos_model(x_train, y_train, features_val, label_val, params):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Add the feature layer (the list of features and how they are represented)\n",
    "    # to the model.\n",
    "    model.add(feature_layer)\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(params['first_neuron'], input_dim=x_train.shape[1],\n",
    "                    activation=params['activation'],\n",
    "                    kernel_initializer=params['kernel_initializer']))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dropout(params['dropout']))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1, activation=params['last_activation'],\n",
    "                    kernel_initializer=params['kernel_initializer']))\n",
    "    \n",
    "    model.compile(loss=params['losses'],\n",
    "                  optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])),\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Manual kfold cross_validation\n",
    "#     kfold = KFold(n_splits=params[num_of_splits], shuffle=True, random_state=7)\n",
    "#     for train_index, validate_index in kfold.split(train_df, train_df['target_2015']):\n",
    "#         print(train_index)\n",
    "#         print(validate_index)\n",
    "#         weight, bias, epochs_hist, rmse = train_model(my_model, train_df.iloc[train_index, :], train_df.iloc[validate_index, :], my_label,\n",
    "#                                                  params[epochs], params[batch_size])\n",
    "\n",
    "    # Create callback for early stop\n",
    "    # Create checkpoint at that point\n",
    "    # I have added val before root_mean_square to capture this\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='root_mean_squared_error', verbose=1, patience=100)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='root_mean_squared_error', verbose=1, save_best_only=True)\n",
    "    \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        callbacks=[talos.utils.live()],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0)\n",
    "\n",
    "    print(\"\\nAll weights for your model are: \"+ str(weight))\n",
    "    print(\"The learned bias for your model is %.4f\\n\" % bias )\n",
    "\n",
    "    plot_the_loss_curve(epochs_hist, rmse)\n",
    "\n",
    "    return history, model\n",
    "\n",
    "# then we can go ahead and set the parameter space\n",
    "p = {'first_neuron':[1,2,3, 4, 5],\n",
    "     'hidden_layers':[0, 1, 2],\n",
    "     'lr':[0.05, 0.01, 0.005, 0.001, 0.0001],\n",
    "     'batch_size': [30],\n",
    "     'epochs': [100],\n",
    "     'dropout': [0],\n",
    "     'num_of_splits': [2, 5, 8, 10],\n",
    "     'kernel_initializer': ['uniform','normal'],\n",
    "     'optimizer': [tf.keras.optimizers.Nadam, tf.keras.optimizers.Adam, tf.keras.optimizers.RMSprop],\n",
    "     'losses': ['mean_squared_error'],\n",
    "     'activation':['relu', 'elu'],\n",
    "     'last_activation': ['sigmoid']}\n",
    "\n",
    "\n",
    "# and run the experiment\n",
    "t = talos.Scan(x=x_train.to_numpy(),\n",
    "               y=label_train,\n",
    "               model=build_talos_model,\n",
    "               params=p,\n",
    "               experiment_name='talos_tuning',\n",
    "               round_limit=10\n",
    "              )\n",
    "print(t)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#@ Building with keras tuner hyper parameter tuning in mind\\n## Refer to https://www.sicara.ai/blog/hyperparameter-tuning-keras-tuner\\n\\n# from tensorflow import keras\\n# from tensorflow.keras import layers\\nfrom kerastuner.tuners import RandomSearch\\n\\n\\ndef build_model(hp):\\n    model = tf.keras.Sequential()\\n    # Add the feature layer (the list of features and how they are represented)\\n    # to the model.\\n    model.add(feature_layer)\\n    \\n    for i in range(hp.Int('num_layers', 2, 20)):\\n        model.add(tf.keras.layers.Dense(units=hp.Int('units',\\n                                        min_value=1,\\n                                        max_value=513,\\n                                        step=32),\\n                           activation='relu'))\\n    \\n    ## Find a way to include cycling through\\n    ## It is currently not included in the documentation, it would be experimentation\\n    model.compile(\\n        optimizer=keras.optimizers.RMSprop(\\n            hp.Choice('learning_rate',\\n                      values=[0.01, 0.001, 0.005, 0.0001])),\\n        loss='sparse_categorical_crossentropy',\\n        metrics=['accuracy'])\\n    return model\\n\\ntuner = RandomSearch(\\n    build_model,\\n    objective='val_accuracy',\\n    max_trials=5,\\n    executions_per_trial=3,\\n    directory='my_dir',\\n    project_name='keras_tuner')\\n\\ntuner.search_space_summary()\\n\\nNUM_EPOCH_SEARCH = 5\\n\\ntuner.search(x, y,\\n             epochs=NUM_EPOCH_SEARCH,\\n             validation_split=0.2\\n            )\\n\\n# Show a summary of the search\\ntuner.results_summary()\\n\\n# Retrieve the best model.\\nbest_model = tuner.get_best_models(num_models=2)\\n# OR\\nbest_model = tuner.get_best_models(num_models=1)[0]\\n\\n# Evaluate the best model.\\nloss, accuracy = best_model.evaluate(x_test, y_test)\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "KERAS AUTO TUNER\n",
    "DO NOT RUN UNLESS TESTED PROPERLY\n",
    "'''\n",
    "\n",
    "'''\n",
    "#@ Building with keras tuner hyper parameter tuning in mind\n",
    "## Refer to https://www.sicara.ai/blog/hyperparameter-tuning-keras-tuner\n",
    "\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    # Add the feature layer (the list of features and how they are represented)\n",
    "    # to the model.\n",
    "    model.add(feature_layer)\n",
    "    \n",
    "    for i in range(hp.Int('num_layers', 2, 20)):\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=1,\n",
    "                                        max_value=513,\n",
    "                                        step=32),\n",
    "                           activation='relu'))\n",
    "    \n",
    "    ## Find a way to include cycling through\n",
    "    ## It is currently not included in the documentation, it would be experimentation\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.RMSprop(\n",
    "            hp.Choice('learning_rate',\n",
    "                      values=[0.01, 0.001, 0.005, 0.0001])),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='my_dir',\n",
    "    project_name='keras_tuner')\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "NUM_EPOCH_SEARCH = 5\n",
    "\n",
    "tuner.search(x, y,\n",
    "             epochs=NUM_EPOCH_SEARCH,\n",
    "             validation_split=0.2\n",
    "            )\n",
    "\n",
    "# Show a summary of the search\n",
    "tuner.results_summary()\n",
    "\n",
    "# Retrieve the best model.\n",
    "best_model = tuner.get_best_models(num_models=2)\n",
    "# OR\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluate the best model.\n",
    "loss, accuracy = best_model.evaluate(x_test, y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(my_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X                              float64\n",
       "Y                              float64\n",
       "target_2015                    float64\n",
       "elevation                      float64\n",
       "precip2014-11-16-2014-11-23    float64\n",
       "                                ...   \n",
       "norm_annual_precip_2015        float64\n",
       "norm_annual_precip_2019        float64\n",
       "norm_total_precip              float64\n",
       "norm_ave_precip_2015           float64\n",
       "norm_ave_precip_2019           float64\n",
       "Length: 94, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.astype({'norm_elevation': 'float64'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         X\n",
      "0     34.3\n",
      "1     34.3\n",
      "2     34.3\n",
      "3     34.3\n",
      "4     34.3\n",
      "...    ...\n",
      "16461 35.9\n",
      "16462 35.9\n",
      "16463 35.9\n",
      "16464 35.9\n",
      "16465 35.9\n",
      "\n",
      "[16466 rows x 1 columns]\n",
      "0       0.0\n",
      "1       0.0\n",
      "2       0.0\n",
      "3       0.0\n",
      "4       0.0\n",
      "         ..\n",
      "16461   0.0\n",
      "16462   0.0\n",
      "16463   0.0\n",
      "16464   0.0\n",
      "16465   0.0\n",
      "Name: target_2015, Length: 16466, dtype: float64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please provide as model inputs either a single array or a list of arrays. You passed: x=       norm_elevation  norm_XY  norm_XY_elevation  norm_total_precip  \\\n5489             -0.2      0.9                0.3                0.1   \n5490             -0.3      0.9                0.3                0.1   \n5491             -0.2      0.9                0.3                0.1   \n5492             -0.1      0.9                0.2                0.1   \n5493             -0.2      1.0                0.2                0.1   \n...               ...      ...                ...                ...   \n16461             0.1      0.1               -0.1               -2.5   \n16462             0.1      0.1               -0.1               -2.5   \n16463             0.1      0.1               -0.1               -2.5   \n16464             0.1      0.2               -0.1               -2.5   \n16465             0.1      0.2               -0.1               -2.5   \n\n       norm_LC_Type1_mode  norm_X/Y  norm_precip2014-11-30-2014-12-07  \\\n5489                 -0.4      -0.2                              -0.3   \n5490                 -0.4      -0.3                              -0.3   \n5491                 -0.4      -0.3                              -0.3   \n5492                 -0.4      -0.3                              -0.3   \n5493                 -0.4      -0.3                              -0.3   \n...                   ...       ...                               ...   \n16461                -0.4      -1.7                               2.7   \n16462                -0.4      -1.7                               2.7   \n16463                -0.4      -1.7                               2.7   \n16464                -0.4      -1.8                               2.7   \n16465                -0.4      -1.8                               2.7   \n\n       norm_precip2019-05-05-2019-05-12  norm_precip2015-03-08-2015-03-15  \\\n5489                               -0.3                              -0.3   \n5490                               -0.3                              -0.3   \n5491                               -0.3                              -0.3   \n5492                               -0.3                              -0.3   \n5493                               -0.3                              -0.3   \n...                                 ...                               ...   \n16461                               2.6                               2.1   \n16462                               2.6                               2.1   \n16463                               2.6                               2.1   \n16464                               2.6                               2.1   \n16465                               2.6                               2.1   \n\n       norm_precip2019-02-24-2019-03-03  norm_precip2014-11-23-2014-11-30  \\\n5489                               -0.4                              -0.3   \n5490                               -0.4                              -0.3   \n5491                               -0.4                              -0.3   \n5492                               -0.4                              -0.3   \n5493                               -0.4                              -0.3   \n...                                 ...                               ...   \n16461                               2.2                               3.3   \n16462                               2.2                               3.3   \n16463                               2.2                               3.3   \n16464                               2.2                               3.3   \n16465                               2.2                               3.3   \n\n       norm_precip2019-02-10-2019-02-17  norm_precip2019-04-21-2019-04-28  \\\n5489                                0.9                              -0.4   \n5490                                0.9                              -0.4   \n5491                                0.9                              -0.4   \n5492                                0.9                              -0.4   \n5493                                0.9                              -0.4   \n...                                 ...                               ...   \n16461                               1.7                               3.0   \n16462                               1.7                               3.0   \n16463                               1.7                               3.0   \n16464                               1.7                               3.0   \n16465                               1.7                               3.0   \n\n       norm_precip2019-03-24-2019-03-31  norm_precip2019-03-31-2019-04-07  \\\n5489                               -0.6                              -0.4   \n5490                               -0.6                              -0.4   \n5491                               -0.6                              -0.4   \n5492                               -0.6                              -0.4   \n5493                               -0.6                              -0.4   \n...                                 ...                               ...   \n16461                               2.5                               3.4   \n16462                               2.5                               3.4   \n16463                               2.5                               3.4   \n16464                               2.5                               3.4   \n16465                               2.5                               3.4   \n\n       norm_precip2014-11-16-2014-11-23  norm_precip2019-05-12-2019-05-19  \\\n5489                               -0.4                              -0.3   \n5490                               -0.4                              -0.3   \n5491                               -0.4                              -0.3   \n5492                               -0.4                              -0.3   \n5493                               -0.4                              -0.3   \n...                                 ...                               ...   \n16461                               3.6                               3.7   \n16462                               3.6                               3.7   \n16463                               3.6                               3.7   \n16464                               3.6                               3.7   \n16465                               3.6                               3.7   \n\n       norm_precip2019-04-28-2019-05-05  norm_precip2019-03-17-2019-03-24  \n5489                                0.5                              -1.1  \n5490                                0.5                              -1.1  \n5491                                0.5                              -1.1  \n5492                                0.5                              -1.1  \n5493                                0.5                              -1.1  \n...                                 ...                               ...  \n16461                               2.9                              -0.7  \n16462                               2.9                              -0.7  \n16463                               2.9                              -0.7  \n16464                               2.9                              -0.7  \n16465                               2.9                              -0.7  \n\n[10977 rows x 19 columns]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-2cbab4f36600>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m                    )\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m \u001b[0mgrid_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmy_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target_2015'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;31m# summarize results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best: %f using %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    685\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    664\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 666\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2543\u001b[0m             not tensor_util.is_tensor(x_input)):\n\u001b[0;32m   2544\u001b[0m           raise ValueError('Please provide as model inputs either a single '\n\u001b[1;32m-> 2545\u001b[1;33m                            'array or a list of arrays. You passed: x=' + str(x))\n\u001b[0m\u001b[0;32m   2546\u001b[0m         \u001b[0mall_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Please provide as model inputs either a single array or a list of arrays. You passed: x=       norm_elevation  norm_XY  norm_XY_elevation  norm_total_precip  \\\n5489             -0.2      0.9                0.3                0.1   \n5490             -0.3      0.9                0.3                0.1   \n5491             -0.2      0.9                0.3                0.1   \n5492             -0.1      0.9                0.2                0.1   \n5493             -0.2      1.0                0.2                0.1   \n...               ...      ...                ...                ...   \n16461             0.1      0.1               -0.1               -2.5   \n16462             0.1      0.1               -0.1               -2.5   \n16463             0.1      0.1               -0.1               -2.5   \n16464             0.1      0.2               -0.1               -2.5   \n16465             0.1      0.2               -0.1               -2.5   \n\n       norm_LC_Type1_mode  norm_X/Y  norm_precip2014-11-30-2014-12-07  \\\n5489                 -0.4      -0.2                              -0.3   \n5490                 -0.4      -0.3                              -0.3   \n5491                 -0.4      -0.3                              -0.3   \n5492                 -0.4      -0.3                              -0.3   \n5493                 -0.4      -0.3                              -0.3   \n...                   ...       ...                               ...   \n16461                -0.4      -1.7                               2.7   \n16462                -0.4      -1.7                               2.7   \n16463                -0.4      -1.7                               2.7   \n16464                -0.4      -1.8                               2.7   \n16465                -0.4      -1.8                               2.7   \n\n       norm_precip2019-05-05-2019-05-12  norm_precip2015-03-08-2015-03-15  \\\n5489                               -0.3                              -0.3   \n5490                               -0.3                              -0.3   \n5491                               -0.3                              -0.3   \n5492                               -0.3                              -0.3   \n5493                               -0.3                              -0.3   \n...                                 ...                               ...   \n16461                               2.6                               2.1   \n16462                               2.6                               2.1   \n16463                               2.6                               2.1   \n16464                               2.6                               2.1   \n16465                               2.6                               2.1   \n\n       norm_precip2019-02-24-2019-03-03  norm_precip2014-11-23-2014-11-30  \\\n5489                               -0.4                              -0.3   \n5490                               -0.4                              -0.3   \n5491                               -0.4                              -0.3   \n5492                               -0.4                              -0.3   \n5493                               -0.4                              -0.3   \n...                                 ...                               ...   \n16461                               2.2                               3.3   \n16462                               2.2                               3.3   \n16463                               2.2                               3.3   \n16464                               2.2                               3.3   \n16465                               2.2                               3.3   \n\n       norm_precip2019-02-10-2019-02-17  norm_precip2019-04-21-2019-04-28  \\\n5489                                0.9                              -0.4   \n5490                                0.9                              -0.4   \n5491                                0.9                              -0.4   \n5492                                0.9                              -0.4   \n5493                                0.9                              -0.4   \n...                                 ...                               ...   \n16461                               1.7                               3.0   \n16462                               1.7                               3.0   \n16463                               1.7                               3.0   \n16464                               1.7                               3.0   \n16465                               1.7                               3.0   \n\n       norm_precip2019-03-24-2019-03-31  norm_precip2019-03-31-2019-04-07  \\\n5489                               -0.6                              -0.4   \n5490                               -0.6                              -0.4   \n5491                               -0.6                              -0.4   \n5492                               -0.6                              -0.4   \n5493                               -0.6                              -0.4   \n...                                 ...                               ...   \n16461                               2.5                               3.4   \n16462                               2.5                               3.4   \n16463                               2.5                               3.4   \n16464                               2.5                               3.4   \n16465                               2.5                               3.4   \n\n       norm_precip2014-11-16-2014-11-23  norm_precip2019-05-12-2019-05-19  \\\n5489                               -0.4                              -0.3   \n5490                               -0.4                              -0.3   \n5491                               -0.4                              -0.3   \n5492                               -0.4                              -0.3   \n5493                               -0.4                              -0.3   \n...                                 ...                               ...   \n16461                               3.6                               3.7   \n16462                               3.6                               3.7   \n16463                               3.6                               3.7   \n16464                               3.6                               3.7   \n16465                               3.6                               3.7   \n\n       norm_precip2019-04-28-2019-05-05  norm_precip2019-03-17-2019-03-24  \n5489                                0.5                              -1.1  \n5490                                0.5                              -1.1  \n5491                                0.5                              -1.1  \n5492                                0.5                              -1.1  \n5493                                0.5                              -1.1  \n...                                 ...                               ...  \n16461                               2.9                              -0.7  \n16462                               2.9                              -0.7  \n16463                               2.9                              -0.7  \n16464                               2.9                              -0.7  \n16465                               2.9                              -0.7  \n\n[10977 rows x 19 columns]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def create_model(dropout_rate=0.0):\n",
    "    \n",
    "#     return model\n",
    " \n",
    "# model = KerasClassifier(build_fn=create_model, dropout_rate=0.2)\n",
    "\n",
    "\n",
    "\n",
    "# param_grid = dict(epochs=[10,20,30])\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "# grid_result = grid.fit(X, Y)\n",
    "\n",
    "\n",
    "\n",
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "# Function to create model, required for KerasRegressor\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Add the feature layer (the list of features and how they are represented)\n",
    "    # to the model.\n",
    "    model.add(feature_layer)\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(12, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', \n",
    "                  optimizer= tf.keras.optimizers.Adam(lr=0.0001), \n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# # load dataset\n",
    "# dataset = train_df\n",
    "# label_name = 'target_2015'\n",
    "\n",
    "# # split into input (X) and output (Y) variables\n",
    "# # X = dataset\n",
    "# # Y = train['target_2015']\n",
    "\n",
    "\n",
    "# X = {name:np.array(value) for name, value in dataset.items()}\n",
    "# Y = np.array(X.pop(label_name))\n",
    "# print(Y.shape)\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "learning_rate = [0.05, 0.01, 0.005, 0.001, 0.0001]\n",
    "params = {'batch_size':(10, 20, 40, 60, 80, 100), 'epochs':(10, 50, 100)}\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=params\n",
    "#                     n_jobs=-1, \n",
    "#                     cv=3\n",
    "                   )\n",
    "\n",
    "grid_result = grid.fit(train_df[my_cols], train_df['target_2015'])\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.01 # originally 0.01\n",
    "epochs = 10000\n",
    "batch_size = 2000\n",
    "\n",
    "# # Invoke the normalize feature method\n",
    "# normalize_feature(\"ave_precip_2015\", \"ave_precip_2019\")\n",
    "\n",
    "my_label=\"target_2015\" # the median value of a house on a specific city block.\n",
    "# That is, you're going to create a model that predicts house value based \n",
    "# solely on total_rooms.  \n",
    "\n",
    "# Discard any pre-existing version of the model.\n",
    "my_model = None\n",
    "\n",
    "# Build the model\n",
    "my_model = build_model(learning_rate)\n",
    "\n",
    "# Build talos optmized model\n",
    "# Manual kfold cross_validation\n",
    "kfold = KFold(n_splits=params[num_of_splits], shuffle=True, random_state=7)\n",
    "for train_index, validate_index in kfold.split(train_df, train_df['target_2015']):\n",
    "    my_talos_model = build_talos_model(train_df.drop('target_2015').iloc[train_index, :], train_df['target_2015'],\n",
    "                                      train_df.drop('target_2015').iloc[validate_index, :], train_df['target_2015'], p)\n",
    "\n",
    "# Manual kfold cross_validation\n",
    "kfold = KFold(n_splits=8, shuffle=True, random_state=7)\n",
    "for train_index, validate_index in kfold.split(train_df, train_df['target_2015']):\n",
    "    print(train_index)\n",
    "    print(validate_index)\n",
    "    weight, bias, epochs_hist, rmse = train_model(my_model, train_df.iloc[train_index, :], train_df.iloc[validate_index, :], my_label,\n",
    "                                             epochs, batch_size)\n",
    "\n",
    "\n",
    "print(\"\\nThe w1 learned weight for your model is %.4f\" % weight[0])\n",
    "print(\"\\nThe w2 learned weight for your model is %.4f\" % weight[1])\n",
    "print(\"\\nAll weights for your model are: \"+ str(weight))\n",
    "print(\"The learned bias for your model is %.4f\\n\" % bias )\n",
    "\n",
    "# plot_the_model_plot(weight, bias, my_feature, my_label)\n",
    "# plot_the_model_plotly(weight[0], weight[1], bias, \"norm_XY_elevation\", \"norm_XY\", my_label)\n",
    "plot_the_loss_curve(epochs_hist, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_target_2015(my_label):\n",
    "  \"\"\"Predict the extent of flooding based on features in the validation set.\"\"\"\n",
    "\n",
    "  features = {name:np.array(value) for name, value in validate_df.items()}\n",
    "  label = np.array(features.pop(my_label))\n",
    "\n",
    "  my_model.evaluate(x = features, y = label, batch_size=batch_size)\n",
    "    \n",
    " \n",
    "# Needs to be reworked\n",
    "def predict_target_2019(label):\n",
    "  \"\"\"Predict the extent of flooding based on a feature.\"\"\"\n",
    "\n",
    "  # Create test set for submission file\n",
    "  test_df = train_df.drop('target_2015')\n",
    "\n",
    "  # Test using the test set\n",
    "  print(\"\\n: Evaluate the new model against the test set:\")\n",
    "  test_features = {name:np.array(value) for name, value in test_df.items()}\n",
    "\n",
    "  batch = test_features\n",
    "  predicted_values = my_model.predict_on_batch(x=batch)\n",
    "\n",
    "  np.savetxt(\"../Source/submission_DNN_multi_feature.csv\", predicted_values, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invoke the target prediction on validate set:\n",
    "predict_target_2015(my_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invoke the target prediction on test set:\n",
    "predict_target_2019(my_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
